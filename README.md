# 🤖 PromptCritic: BERT-Based Answer Quality Classifier for LLM Outputs

PromptCritic is a lightweight yet powerful deep learning tool for automatically evaluating the quality of answers generated by large language models (LLMs). Built on PyTorch and fine-tuned from BERT, it classifies responses into three categories — **Low**, **Medium**, and **High** — and provides a foundation for prompt optimization, dataset filtering, and research on LLM evaluation.

---

## 📌 Features

- ✅ Fine-tuned BERT classifier (`bert-base-uncased`)
- ✅ 3-class classification: Low (0), Medium (1), High (2)
- ✅ Based on [UltraFeedback dataset](https://huggingface.co/datasets/openbmb/UltraFeedback)
- ✅ Supports evaluation, batch prediction, and visualization
- ✅ Easy to retrain or extend (supports early stopping, weighted loss)

---

## 🧠 Quality Evaluation Criteria

Each answer is rated based on the following 5 factors:

1. ✅ **Answered the question** (Yes / No)  
2. 🧠 **Factual accuracy** (Yes / No, optionally verified with search)  
3. 🗣️ **Clarity and conciseness** (Good / Fair / Poor)  
4. 🧱 **Logical structure** (Yes / No)  
5. 📏 **Length or redundancy issues** (Yes / No)  

Final label:
- **0 = Low quality**: hallucinated, off-topic, incoherent
- **1 = Medium quality**: partially correct or verbose
- **2 = High quality**: accurate, helpful, clear

---

## 🗂 Project Structure

```

PromptCritic/
├── data/
│   └── 6000.csv                   # Final balanced dataset (2000 per class)
├── models/
│   └── bert-critic-1.0/          # Final saved model (HuggingFace format)
├── scripts/
│   ├── convert_ultrafeedback.py  # Convert UltraFeedback to labeled CSV
│   ├── dataset_builder.py        # Dataset + tokenizer processor
│   ├── train.py                  # Training script (baseline)
│   ├── train_V2.py               # Training with early stopping & class weights
│   ├── evaluate.py               # Model evaluation & confusion matrix
│   ├── predict.py                # Inference on custom inputs or batch
│   ├── frequency.py              # Label frequency analysis
│   ├── small_sample.py           # Equal sample per label
│   └── small_sample_random.py    # Random sample preserving label ratio
├── results/
│   └── confusion_matrix.png      # Evaluation visualization 
├── requirements.txt
└── README.md

````

---

## 🧪 Quick Start

### 1. Install requirements
```bash
pip install -r requirements.txt
````

### 2. Train the model

```bash
python scripts/train.py \
    --csv_path data/6000.csv \
    --model_name bert-base-uncased \
    --epochs 4 \
    --batch_size 16 \
    --lr 2e-5 \
    --output_dir models/bert-critic-1.0
```

### 3. Evaluate

```bash
python scripts/evaluate.py \
    --model_dir models/bert-critic-1.0 \
    --csv_path data/6000.csv
```

### 4. Predict

```bash
python scripts/predict.py \
    --model_dir models/bert-critic-1.0 \
    --csv_path your_prompts.csv
```

---

## 🎯 Results (on 6000 balanced samples)

| Class       | Precision | Recall | F1 Score           |
| ----------- | --------- | ------ | ------------------ |
| Low (0)     | 0.764     | 0.863  | 0.811              |
| Medium (1)  | 0.766     | 0.597  | 0.671              |
| High (2)    | 0.741     | 0.809  | 0.774              |
| **Overall** |           |        | **0.756 Accuracy** |

👉 Confusion matrix and classification report are saved to `results/confusion_matrix.png`.

---

## 🧩 Dataset Summary

* Source: [UltraFeedback (openbmb)](https://huggingface.co/datasets/openbmb/UltraFeedback)
* Final subset: 6000 samples (2000 per label)
* Converted and labeled via script: `convert_ultrafeedback.py`
* Balanced and cleaned, suitable for reproducible training

---

## 🗃 Model Download

The final model is available at:

📁 `models/bert-critic-1.0/`
Use with HuggingFace Transformers:

```python
from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained("models/bert-critic-1.0")
```

---

## 📌 Future Plans

* [ ] Streamlit Web Demo (LLM+Critic interaction)
* [ ] Labeling assistance with GPT feedback loops
* [ ] Expand to multilingual responses or GPT-4 outputs
* [ ] Regression-based fine-grained scoring

---

## 📜 License

This project is released under the MIT License. See `LICENSE` for details.

---

## 🙏 Acknowledgements

* [OpenBMB](https://github.com/OpenBMB) for the UltraFeedback dataset
* HuggingFace Transformers for pre-trained models
* Community members for insights on LLM evaluation

---

Feel free to ⭐ this repo if you find it useful!
