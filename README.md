# ğŸ¤– PromptCritic: BERT-Based Answer Quality Classifier for LLM Outputs

PromptCritic is a lightweight yet powerful deep learning tool for automatically evaluating the quality of answers generated by large language models (LLMs). Built on PyTorch and fine-tuned from BERT, it classifies responses into three categories â€” **Low**, **Medium**, and **High** â€” and provides a foundation for prompt optimization, dataset filtering, and research on LLM evaluation.

---

## ğŸ“Œ Features

- âœ… Fine-tuned BERT classifier (`bert-base-uncased`)
- âœ… 3-class classification: Low (0), Medium (1), High (2)
- âœ… Based on [UltraFeedback dataset](https://huggingface.co/datasets/openbmb/UltraFeedback)
- âœ… Supports evaluation, batch prediction, and visualization
- âœ… Easy to retrain or extend (supports early stopping, weighted loss)

---

## ğŸ§  Quality Evaluation Criteria

Each answer is rated based on the following 5 factors:

1. âœ… **Answered the question** (Yes / No)  
2. ğŸ§  **Factual accuracy** (Yes / No, optionally verified with search)  
3. ğŸ—£ï¸ **Clarity and conciseness** (Good / Fair / Poor)  
4. ğŸ§± **Logical structure** (Yes / No)  
5. ğŸ“ **Length or redundancy issues** (Yes / No)  

Final label:
- **0 = Low quality**: hallucinated, off-topic, incoherent
- **1 = Medium quality**: partially correct or verbose
- **2 = High quality**: accurate, helpful, clear

---

## ğŸ—‚ Project Structure

```

PromptCritic/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ 6000.csv                   # Final balanced dataset (2000 per class)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ bert-critic-1.0/          # Final saved model (HuggingFace format)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ convert_ultrafeedback.py  # Convert UltraFeedback to labeled CSV
â”‚   â”œâ”€â”€ dataset_builder.py        # Dataset + tokenizer processor
â”‚   â”œâ”€â”€ train.py                  # Training script (baseline)
â”‚   â”œâ”€â”€ train_V2.py               # Training with early stopping & class weights
â”‚   â”œâ”€â”€ evaluate.py               # Model evaluation & confusion matrix
â”‚   â”œâ”€â”€ predict.py                # Inference on custom inputs or batch
â”‚   â”œâ”€â”€ frequency.py              # Label frequency analysis
â”‚   â”œâ”€â”€ small_sample.py           # Equal sample per label
â”‚   â””â”€â”€ small_sample_random.py    # Random sample preserving label ratio
â”œâ”€â”€ results/
â”‚   â””â”€â”€ confusion_matrix.png      # Evaluation visualization 
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ§ª Quick Start

### 1. Install requirements
```bash
pip install -r requirements.txt
````

### 2. Train the model

```bash
python scripts/train.py \
    --csv_path data/6000.csv \
    --model_name bert-base-uncased \
    --epochs 4 \
    --batch_size 16 \
    --lr 2e-5 \
    --output_dir models/bert-critic-1.0
```

### 3. Evaluate

```bash
python scripts/evaluate.py \
    --model_dir models/bert-critic-1.0 \
    --csv_path data/6000.csv
```

### 4. Predict

```bash
python scripts/predict.py \
    --model_dir models/bert-critic-1.0 \
    --csv_path your_prompts.csv
```

---

## ğŸ¯ Results (on 6000 balanced samples)

| Class       | Precision | Recall | F1 Score           |
| ----------- | --------- | ------ | ------------------ |
| Low (0)     | 0.764     | 0.863  | 0.811              |
| Medium (1)  | 0.766     | 0.597  | 0.671              |
| High (2)    | 0.741     | 0.809  | 0.774              |
| **Overall** |           |        | **0.756 Accuracy** |

ğŸ‘‰ Confusion matrix and classification report are saved to `results/confusion_matrix.png`.

---

## ğŸ§© Dataset Summary

* Source: [UltraFeedback (openbmb)](https://huggingface.co/datasets/openbmb/UltraFeedback)
* Final subset: 6000 samples (2000 per label)
* Converted and labeled via script: `convert_ultrafeedback.py`
* Balanced and cleaned, suitable for reproducible training

---

## ğŸ—ƒ Model Download

The final model is available at:

ğŸ“ `models/bert-critic-1.0/`
Use with HuggingFace Transformers:

```python
from transformers import BertForSequenceClassification
model = BertForSequenceClassification.from_pretrained("models/bert-critic-1.0")
```

---

## ğŸ“Œ Future Plans

* [ ] Streamlit Web Demo (LLM+Critic interaction)
* [ ] Labeling assistance with GPT feedback loops
* [ ] Expand to multilingual responses or GPT-4 outputs
* [ ] Regression-based fine-grained scoring

---

## ğŸ“œ License

This project is released under the MIT License. See `LICENSE` for details.

---

## ğŸ™ Acknowledgements

* [OpenBMB](https://github.com/OpenBMB) for the UltraFeedback dataset
* HuggingFace Transformers for pre-trained models
* Community members for insights on LLM evaluation

---

Feel free to â­ this repo if you find it useful!
