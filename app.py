import streamlit as st
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# ------------------- åŠ è½½æ¨¡å‹ ------------------- #
@st.cache_resource
def load_model(model_dir):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    model.to(device)
    model.eval()
    return tokenizer, model, device

# ------------------- å•æ¡é¢„æµ‹ ------------------- #
def predict_single(prompt, response, tokenizer, model, device):
    text = prompt.strip() + "\n\n" + response.strip()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=-1).squeeze().cpu().tolist()
    label = int(torch.argmax(torch.tensor(probs)).item())
    return label, probs

# ------------------- é¡µé¢è®¾ç½® ------------------- #
st.set_page_config("PromptCritic å¯è§†åŒ–", layout="wide")
st.title("ğŸ§  PromptCritic - å›ç­”è´¨é‡è¯„ä¼°å™¨")
model_dir = st.sidebar.text_input("æ¨¡å‹ç›®å½•", value="models/bert-critic-1.0/best_model")
tokenizer, model, device = load_model(model_dir)

# ------------------- è¾“å…¥åŒºåŸŸ ------------------- #
tabs = st.tabs(["ğŸ“Œ å•æ¡è¾“å…¥", "ğŸ“ æ‰¹é‡ä¸Šä¼ "])

with tabs[0]:
    st.subheader("ğŸ“Œ å•æ¡ prompt + response é¢„æµ‹")
    prompt = st.text_area("ğŸ“ Prompt", height=120)
    response = st.text_area("ğŸ’¬ Response", height=200)

    if st.button("ğŸ” é¢„æµ‹è´¨é‡ç­‰çº§"):
        if not prompt or not response:
            st.warning("è¯·å¡«å†™å®Œæ•´ Prompt å’Œ Responseï¼")
        else:
            label, probs = predict_single(prompt, response, tokenizer, model, device)
            label_map = {0: "âŒ ä½è´¨é‡", 1: "âš ï¸ ä¸­ç­‰è´¨é‡", 2: "âœ… é«˜è´¨é‡"}
            st.markdown(f"### ğŸ¯ æ¨¡å‹åˆ¤æ–­ç»“æœï¼š{label_map[label]}")
            st.markdown("### ğŸ“Š æ¦‚ç‡åˆ†å¸ƒ")
            df_probs = pd.DataFrame({"label": ["ä½ (0)", "ä¸­ (1)", "é«˜ (2)"], "prob": probs})
            fig, ax = plt.subplots()
            sns.barplot(x="label", y="prob", data=df_probs, palette="Blues_d", ax=ax)
            ax.set_ylim(0, 1)
            st.pyplot(fig)

with tabs[1]:
    st.subheader("ğŸ“ æ‰¹é‡ä¸Šä¼  CSV æ–‡ä»¶ (éœ€åŒ…å« 'prompt', 'response' åˆ—)")
    uploaded_file = st.file_uploader("é€‰æ‹© CSV æ–‡ä»¶", type="csv")

    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        if 'prompt' not in df.columns or 'response' not in df.columns:
            st.error("âŒ CSV æ–‡ä»¶ä¸­å¿…é¡»åŒ…å« 'prompt' å’Œ 'response' åˆ—ï¼")
        else:
            st.write("ğŸ“„ åŸå§‹æ•°æ®é¢„è§ˆï¼š", df.head())
            labels, probs0, probs1, probs2 = [], [], [], []
            for i, row in stqdm(df.iterrows(), total=len(df), desc="é¢„æµ‹ä¸­..."):
                label, probs = predict_single(row['prompt'], row['response'], tokenizer, model, device)
                labels.append(label)
                probs0.append(probs[0]); probs1.append(probs[1]); probs2.append(probs[2])
            df['pred_label'] = labels
            df['prob_low'] = probs0
            df['prob_med'] = probs1
            df['prob_high'] = probs2
            st.success("âœ… æ‰¹é‡é¢„æµ‹å®Œæˆï¼")
            st.dataframe(df)
            csv = df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ CSV", csv, file_name="predicted_results.csv", mime="text/csv")
